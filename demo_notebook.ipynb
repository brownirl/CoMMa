{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from limp.utils.gen_utils import ltl2dfa, llm4tl, referent_verification, task_structure_verification, get_spatial_referents\n",
    "from osg.utils.general_utils import load_data, create_observation_graph\n",
    "from osg.vlm_library import vlm_library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_fldr=f\"results/\"\n",
    "vlm_instance   = vlm_library(vl_model=\"owl_vit\",  seg_model=\"mobile_sam\", tmp_fldr=tmp_fldr) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/spot_room\"\n",
    "\n",
    "observation_data, edge_connectivity, env_pointcloud = load_data(data_path, tmp_fldr)\n",
    "observations_graph, _, _, _ = create_observation_graph(observation_data,edge_connectivity,tmp_fldr=tmp_fldr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language to  Ltl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n*************************************************************************\\nInstruction Following\\n*************************************************************************\")\n",
    "referring_expressions, task_ltl = None, None\n",
    "input_lang_instruction = input(\"Enter the natural language description of the task: \")\n",
    "\n",
    "in_context_examples = \"limp/language/temporal_logic/ltl_datasets/efficient-eng-2-ltl-droneplanning\"\n",
    "in_context_count    = 10\n",
    "lang2ltl_path       = \"limp/language/temporal_logic/embedding_cache/small-droneplanning_lang2ltl.pkl\" \n",
    "lang2embedding_path = \"limp/language/temporal_logic/embedding_cache/small-droneplanning_lang2embeddings.pkl\"\n",
    "\n",
    "print(f'\\nInput instruction: \"{input_lang_instruction}\"')\n",
    "print(\"Running Language Instruction Module ...\")\n",
    "strategy_choice=\"two_stage_similar_embedding\"\n",
    "# strategy_choice=\"two_stage_random_embedding\"\n",
    "# strategy_choice=\"single_stage\"\n",
    "\n",
    "encoding_map, response_ltl, spot_ltl, llm_response_history= llm4tl(input_lang_instruction, in_context_examples, lang2embedding_path, lang2ltl_path, in_context_count, enable_prints=False, strategy=strategy_choice)\n",
    "original_encoding_map, original_response_ltl, original_spot_ltl, original_llm_response_history = encoding_map, response_ltl, spot_ltl, llm_response_history \n",
    "\n",
    "print(\"Spotify predicate encoding map: \", encoding_map)\n",
    "print(\"Response LTL formula: \", response_ltl)\n",
    "print(\"Cleaned LTL formula: \", spot_ltl,\"\\n\")\n",
    "\n",
    "display(spot_ltl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Symbol Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referent verification\n",
    "encoding_map, response_ltl, spot_ltl, llm_response_history  = referent_verification(input_lang_instruction, encoding_map, response_ltl, spot_ltl, llm_response_history, strategy_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task structure verification\n",
    "encoding_map, response_ltl, spot_ltl, llm_response_history, selected_dfa_path  = task_structure_verification(input_lang_instruction, encoding_map, response_ltl, spot_ltl, llm_response_history, strategy_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize original and verified results\n",
    "print(\"*****************************************\\nOriginal Results\\n*****************************************\")\n",
    "print(\"Original encoded formula: \",original_spot_ltl)\n",
    "print(\"Original encoding map: \",original_encoding_map)\n",
    "print(\"*****************************************\\nAfter Verification\\n*****************************************\")\n",
    "print(\"Verified encoded formula: \",spot_ltl)\n",
    "print(\"Verified encoding map: \",encoding_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct task dfa from ltl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constructing task dfa from ltl formula\n",
    "task_dfa, dfa_graph = ltl2dfa(encoding_map,spot_ltl, visualize_details=True, show_diagram=True, show_labels=True, path=selected_dfa_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground referents and filter instances via spatial constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract spatial information\n",
    "referent_spatial_details = get_spatial_referents(encoding_map)\n",
    "print(\"referent_spatial_details: \",referent_spatial_details,\"\\n\")\n",
    "\n",
    "## Spatial grounding\n",
    "relevant_element_details = vlm_instance.spatial_grounding(observations_graph, referent_spatial_details, visualize=True, use_segmentation=True, multiprocessing=True, workers=3)\n",
    "\n",
    "print(\"Referents after spatial constraint filtering:\",len(relevant_element_details))\n",
    "#for all relevant elements print their ids\n",
    "print(f\"\\nFiltered elements \\n\",[element['mask_id'] for element in relevant_element_details])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select Robot Start Point in Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from limp.planner.multi_level_planner import grid_to_world, world_to_grid, generate_obstacle_map\n",
    "from limp.utils.fmt_utils import plot_map_with_points\n",
    "%matplotlib widget\n",
    "\n",
    "resolution = 0.01\n",
    "h_min_bottom = -3\n",
    "h_max_top = 1\n",
    "obstacle_map, _, map_min_bound, map_max_bound = generate_obstacle_map(env_pointcloud, None, resolution, h_min_bottom,  h_max_top)\n",
    "\n",
    "## If we visually want to get start point from map\n",
    "clicked_points = plot_map_with_points(obstacle_map)\n",
    "\n",
    "## If we know real world coordinates of start point\n",
    "# world_coords = [0,0]\n",
    "# grid_start_coords = world_to_grid(world_coords, map_min_bound, resolution)\n",
    "# print(f\"World Cordinate: {world_coords} || Start point in grid: {grid_start_coords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progressive motion planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from limp.planner.multi_level_planner import progressive_motion_planner\n",
    "\n",
    "robot_motion_type = \"3D\"              #determines if planing space is 2D or 3D\n",
    "z_height_2d=0                         #height of 2D planning space\n",
    "      \n",
    "step_factor=40                        #determines density of generated visual demarcations of regions of interest. Adjust for denser or sparser points\n",
    "goal_sampling_percentage=20           #percentage of goal points to sample from the goal region to make planning tractable\n",
    "use_heuristic_flag=True               #determines if we use modified version of FMT* with cost to goal heuristic \n",
    "visualize_flag=True                   #determines if we visualize the computed motion plan\n",
    "obstacle_map_resolution = 0.01        #determines the resolution of the obstacle map\n",
    "filter_h_min_bottom = -3              #height to filter out pointcloud points below this height (meters) [Floor]      \n",
    "filter_h_max_top= 1                   #height to filter out pointcloud points above this height (meters) [Roof] #see doors: 0.7 || Old value:-0.15\n",
    "nearness_threshold = 1                #determines the meaning of nearness of planning space demarcation (meters)\n",
    "start_point = clicked_points[-1]      #robot start location\n",
    "show_color_bars_flag = False\n",
    "\n",
    "computed_plan = progressive_motion_planner(start_point, task_dfa, dfa_graph, env_pointcloud, relevant_element_details, encoding_map, nearness_threshold, obstacle_map_resolution, filter_h_min_bottom, filter_h_max_top, robot_motion_type, height_2d=z_height_2d, stepfactor=step_factor, use_heuristic=use_heuristic_flag,visualize=visualize_flag,tmp_fldr=tmp_fldr,goal_sample_percentage=goal_sampling_percentage,show_color_bars=show_color_bars_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Task and Motion Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_plan['world_plan'] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlgmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
